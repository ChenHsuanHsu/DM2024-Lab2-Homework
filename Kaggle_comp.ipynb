{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import (\n",
    "    VotingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change environment when running in different terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXEC_ENV_ENUM(Enum):\n",
    "    COLAB = 1\n",
    "    KAGGLE = 2\n",
    "    LOCAL = 3\n",
    "\n",
    "\"\"\"\n",
    "‰∏üÂà∞ kaggle Ë¶ÅÊîπÁöÑÂú∞Êñπ\n",
    "\"\"\"\n",
    "# EXEC_ENV = EXEC_ENV_ENUM.COLAB\n",
    "# EXEC_ENV = EXEC_ENV_ENUM.KAGGLE\n",
    "EXEC_ENV = EXEC_ENV_ENUM.LOCAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sorting and combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Â≠òÂÇ®ÊèêÂèñÊï∞ÊçÆÁöÑÂàóË°®\n",
    "tweets_data = []\n",
    "\n",
    "# ËØªÂèñ JSON Êñá‰ª∂\n",
    "with open('dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Ëß£Êûê JSON ÊØèË°å\n",
    "            tweet = json.loads(line)\n",
    "            \n",
    "            # ÊèêÂèñÂ≠óÊÆµ\n",
    "            tweet_id = tweet[\"_source\"][\"tweet\"].get(\"tweet_id\")\n",
    "            text = tweet[\"_source\"][\"tweet\"].get(\"text\")\n",
    "            hashtags = tweet[\"_source\"][\"tweet\"].get(\"hashtags\", [])\n",
    "            crawldate = tweet.get(\"_crawldate\")\n",
    "            score = tweet.get(\"_score\")\n",
    "            index = tweet.get(\"_index\")\n",
    "            doc_type = tweet.get(\"_type\")\n",
    "            \n",
    "            # Ë°çÁîüÂ≠óÊÆµ\n",
    "            hashtag_count = len(hashtags)\n",
    "            text_length = len(text) if text else 0\n",
    "            contains_keyword = \"<LH>\" in text if text else False\n",
    "\n",
    "            # Â∞ÜÊèêÂèñÁöÑÂ≠óÊÆµÂ≠òÂÖ•Â≠óÂÖ∏ÔºåÊ∑ªÂä†Âà∞ÂàóË°®\n",
    "            tweets_data.append({\n",
    "                \"tweet_id\": tweet_id,\n",
    "                \"text\": text,\n",
    "                \"hashtags\": hashtags,\n",
    "                \"crawldate\": crawldate,\n",
    "                \"_score\": score,\n",
    "                \"_index\": index,\n",
    "                \"_type\": doc_type,\n",
    "                \"hashtag_count\": hashtag_count,\n",
    "                \"text_length\": text_length,\n",
    "                \"contains_keyword\": contains_keyword\n",
    "            })\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"JSON decode error:\", e)\n",
    "\n",
    "# ËΩ¨Êç¢‰∏∫ DataFrame\n",
    "df = pd.DataFrame(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = pd.read_csv('dm-2024-isa-5810-lab-2-homework/data_identification.csv')  # ÂåÖÂê´ tweet_id Âíå identification\n",
    "emotion = pd.read_csv('dm-2024-isa-5810-lab-2-homework/emotion.csv')  # ÂåÖÂê´ tweet_id Âíå emotion\n",
    "data = pd.merge(df, classify, on='tweet_id', how='inner')\n",
    "data = pd.merge(data, emotion, on='tweet_id', how='outer')\n",
    "# data # Â∞áÊÉÖÁ∑íÁâπÂæµÂèäË≥áÊñôÂàÜÈ°ûÊ¨Ñ‰ΩçÂä†ÂÖ•Áõ∏Âêådataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â∞á data_identification ‰∏≠ identification Ê¨Ñ‰ΩçÂÄºÁÇ∫ 'train' ÁöÑÈÅ∏Âá∫‰æÜ‰ΩúÁÇ∫Ë®ìÁ∑¥ÈõÜ\n",
    "train_data = data[data['identification'] == 'train']\n",
    "\n",
    "# Â∞á identification Ê¨Ñ‰ΩçÂÄºÁÇ∫ 'test' ÁöÑÈÅ∏Âá∫‰æÜ‰ΩúÁÇ∫Ê∏¨Ë©¶ÈõÜ\n",
    "test_data = data[data['identification'] == 'test']\n",
    "\n",
    "# ÊâÄÊúâÊÉÖÊÑüemotion in emotion.csv\n",
    "emotions = data['emotion'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "n_splits = 5  # Statified K-Fold ÁöÑ fold Êï∏\n",
    "LGBM_Params = {\n",
    "    \"learning_rate\": 0.046,\n",
    "    \"max_depth\": 12,\n",
    "    \"num_leaves\": 478,\n",
    "    \"min_data_in_leaf\": 13,\n",
    "    \"feature_fraction\": 0.893,\n",
    "    \"bagging_fraction\": 0.784,\n",
    "    \"bagging_freq\": 4,\n",
    "    \"lambda_l1\": 10,  # Increased from 6.59\n",
    "    \"lambda_l2\": 0.01,  # Increased from 2.68e-06\n",
    "    # 'device': 'gpu'\n",
    "}\n",
    "XGB_Params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"n_estimators\": 200,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 1,  # Increased from 0.1\n",
    "    \"reg_lambda\": 5,  # Increased from 1\n",
    "    \"random_state\": SEED,\n",
    "    # 'tree_method': 'gpu_hist',\n",
    "}\n",
    "CatBoost_Params_original = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 6,\n",
    "    \"iterations\": 200,\n",
    "    \"random_seed\": SEED,\n",
    "    \"verbose\": 0,\n",
    "    \"l2_leaf_reg\": 10,  # Increase this value\n",
    "    # 'task_type': 'GPU'\n",
    "}\n",
    "if EXEC_ENV != EXEC_ENV_ENUM.LOCAL:  # Enable GPU\n",
    "    LGBM_Params['device'] = 'gpu'\n",
    "    XGB_Params['tree_method'] = 'gpu_hist'\n",
    "    CatBoost_Params_original['task_type'] = 'GPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## save to pickle file\n",
    "train_data.to_pickle(\"train_data.pkl\") \n",
    "test_data.to_pickle(\"test_data.pkl\")\n",
    "\n",
    "## load a pickle file\n",
    "train_data = pd.read_pickle(\"train_data.pkl\")\n",
    "test_data = pd.read_pickle(\"test_data.pkl\")\n",
    "# already check the size is 411972"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_type</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>text_length</th>\n",
       "      <th>contains_keyword</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f10</td>\n",
       "      <td>o m g Shut Up And Dance though #BlackMirror &lt;LH&gt;</td>\n",
       "      <td>[BlackMirror]</td>\n",
       "      <td>2015-05-16 10:36:47</td>\n",
       "      <td>242</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f11</td>\n",
       "      <td>On #twitch &lt;LH&gt; on the #Destinybeta #Destiny #...</td>\n",
       "      <td>[twitch, Destinybeta, Destiny, Destiny2, Desti...</td>\n",
       "      <td>2016-10-15 20:46:37</td>\n",
       "      <td>915</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>12</td>\n",
       "      <td>136</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1c7f14</td>\n",
       "      <td>A nice sunny wak this morning not many &lt;LH&gt; ar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-07-04 07:22:56</td>\n",
       "      <td>939</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x1c7f15</td>\n",
       "      <td>I'm one of those people who love candy corn......</td>\n",
       "      <td>[Confession, NationalCandyCornDay, CouldEatThe...</td>\n",
       "      <td>2016-04-16 12:53:40</td>\n",
       "      <td>181</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>5</td>\n",
       "      <td>135</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x1c7f16</td>\n",
       "      <td>@metmuseum What are these? They look like some...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-04-22 17:50:28</td>\n",
       "      <td>970</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>0x38fe18</td>\n",
       "      <td>@LJPBR @FifthHarmony Um  My vote For @FifthHar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-12-06 11:10:57</td>\n",
       "      <td>922</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x38fe19</td>\n",
       "      <td>Where is #WesHoolahan?!  #WALvIRL #COYBIG &lt;LH&gt;</td>\n",
       "      <td>[WesHoolahan, WALvIRL, COYBIG]</td>\n",
       "      <td>2015-02-01 18:04:28</td>\n",
       "      <td>77</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x38fe1a</td>\n",
       "      <td>@mattmfm Fake news! &lt;LH&gt; propagated by Tumpkin...</td>\n",
       "      <td>[not, maga]</td>\n",
       "      <td>2016-12-20 17:19:58</td>\n",
       "      <td>25</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x38fe1c</td>\n",
       "      <td>..today was brutal  ..#Hungover</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-09-13 06:31:27</td>\n",
       "      <td>639</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x38fe1d</td>\n",
       "      <td>Love it when I sun burn my forehead!! NOT!! üò´üò±...</td>\n",
       "      <td>[redheadproblems, ouch, burnt]</td>\n",
       "      <td>2016-09-19 14:35:01</td>\n",
       "      <td>630</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>3</td>\n",
       "      <td>89</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "1        0x1c7f10   o m g Shut Up And Dance though #BlackMirror <LH>   \n",
       "2        0x1c7f11  On #twitch <LH> on the #Destinybeta #Destiny #...   \n",
       "5        0x1c7f14  A nice sunny wak this morning not many <LH> ar...   \n",
       "6        0x1c7f15  I'm one of those people who love candy corn......   \n",
       "7        0x1c7f16  @metmuseum What are these? They look like some...   \n",
       "...           ...                                                ...   \n",
       "1867529  0x38fe18  @LJPBR @FifthHarmony Um  My vote For @FifthHar...   \n",
       "1867530  0x38fe19     Where is #WesHoolahan?!  #WALvIRL #COYBIG <LH>   \n",
       "1867531  0x38fe1a  @mattmfm Fake news! <LH> propagated by Tumpkin...   \n",
       "1867533  0x38fe1c                    ..today was brutal  ..#Hungover   \n",
       "1867534  0x38fe1d  Love it when I sun burn my forehead!! NOT!! üò´üò±...   \n",
       "\n",
       "                                                  hashtags  \\\n",
       "1                                            [BlackMirror]   \n",
       "2        [twitch, Destinybeta, Destiny, Destiny2, Desti...   \n",
       "5                                                       []   \n",
       "6        [Confession, NationalCandyCornDay, CouldEatThe...   \n",
       "7                                                       []   \n",
       "...                                                    ...   \n",
       "1867529                                                 []   \n",
       "1867530                     [WesHoolahan, WALvIRL, COYBIG]   \n",
       "1867531                                        [not, maga]   \n",
       "1867533                                                 []   \n",
       "1867534                     [redheadproblems, ouch, burnt]   \n",
       "\n",
       "                   crawldate  _score          _index   _type  hashtag_count  \\\n",
       "1        2015-05-16 10:36:47     242  hashtag_tweets  tweets              1   \n",
       "2        2016-10-15 20:46:37     915  hashtag_tweets  tweets             12   \n",
       "5        2016-07-04 07:22:56     939  hashtag_tweets  tweets              0   \n",
       "6        2016-04-16 12:53:40     181  hashtag_tweets  tweets              5   \n",
       "7        2017-04-22 17:50:28     970  hashtag_tweets  tweets              0   \n",
       "...                      ...     ...             ...     ...            ...   \n",
       "1867529  2016-12-06 11:10:57     922  hashtag_tweets  tweets              0   \n",
       "1867530  2015-02-01 18:04:28      77  hashtag_tweets  tweets              3   \n",
       "1867531  2016-12-20 17:19:58      25  hashtag_tweets  tweets              2   \n",
       "1867533  2016-09-13 06:31:27     639  hashtag_tweets  tweets              0   \n",
       "1867534  2016-09-19 14:35:01     630  hashtag_tweets  tweets              3   \n",
       "\n",
       "         text_length  contains_keyword identification       emotion  \n",
       "1                 48              True          train           joy  \n",
       "2                136              True          train  anticipation  \n",
       "5                126              True          train           joy  \n",
       "6                135              True          train           joy  \n",
       "7                102              True          train       disgust  \n",
       "...              ...               ...            ...           ...  \n",
       "1867529           83              True          train       sadness  \n",
       "1867530           46              True          train  anticipation  \n",
       "1867531           64              True          train      surprise  \n",
       "1867533           31             False          train       disgust  \n",
       "1867534           89              True          train       sadness  \n",
       "\n",
       "[1455563 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## save to pickle file\n",
    "train_data.to_pickle(\"train_data.pkl\") \n",
    "test_data.to_pickle(\"test_data.pkl\")\n",
    "\n",
    "## load a pickle file\n",
    "train_data = pd.read_pickle(\"train_data.pkl\")\n",
    "test_data = pd.read_pickle(\"test_data.pkl\")\n",
    "train_data # already check the size is 411972"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_data['text'])\n",
    "\n",
    "X_train = BOW_500.transform(train_data['text'])\n",
    "\n",
    "y_train = train_data['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     o m g Shut Up And Dance though #BlackMirror <LH>\n",
      "2    On #twitch <LH> on the #Destinybeta #Destiny #...\n",
      "5    A nice sunny wak this morning not many <LH> ar...\n",
      "6    I'm one of those people who love candy corn......\n",
      "7    @metmuseum What are these? They look like some...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_data[\"text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.543294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120862\n",
      "[LightGBM] [Info] Number of data points in the train set: 1164450, number of used features: 500\n",
      "[LightGBM] [Info] Start training from score 3.765624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "def TrainML(model_class, train_data, test_data):\n",
    "    # Á¢∫‰øùÊñáÊú¨Êï∏ÊìöÊòØÂ≠óÁ¨¶‰∏≤\n",
    "    train_data[\"text\"] = train_data[\"text\"].astype(str)\n",
    "\n",
    "    # ÊñáÊú¨ÂêëÈáèÂåñ\n",
    "    vectorizer = TfidfVectorizer(max_features=500)  # ÂèØ‰ª•Ë™øÊï¥ max_features\n",
    "    X = vectorizer.fit_transform(train_data[\"text\"]).toarray()\n",
    "\n",
    "    # Ê®ôÁ±§Á∑®Á¢º\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(train_data[\"emotion\"])\n",
    "\n",
    "    \"\"\"\n",
    "    ‰ΩøÁî® Stratified K-Fold ÂàÜÂ±§ÊäΩÊ®£Á¢∫‰øùÂêÑÈ°ûÂà•ÊØî‰æã‰∏ÄËá¥\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Ê®°ÂûãË®ìÁ∑¥\n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # ÂÖãÈöÜÊ®°Âûã‰∏¶Ë®ìÁ∑¥\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Validation Score:\", model.score(X_val, y_val))\n",
    "\n",
    "    # ‰ΩøÁî®Êï¥ÂÄãË®ìÁ∑¥ÈõÜÈÄ≤Ë°åÊúÄÁµÇÊ®°ÂûãË®ìÁ∑¥\n",
    "    model_class.fit(X, y)\n",
    "\n",
    "    # Ê∏¨Ë©¶ÈõÜÈ†êÊ∏¨\n",
    "    test_data[\"text\"] = test_data[\"text\"].astype(str)\n",
    "    X_test = vectorizer.transform(test_data[\"text\"]).toarray()\n",
    "    return model_class.predict(X_test)\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# ÂàùÂßãÂåñÂü∫Á§éÊ®°Âûã\n",
    "Light = LGBMRegressor()\n",
    "XGBoost = XGBRegressor()\n",
    "RandomForest = RandomForestRegressor()\n",
    "\n",
    "# ÂÆöÁæ©ÊäïÁ•®ÂõûÊ≠∏Ê®°Âûã\n",
    "voting_model = VotingRegressor(\n",
    "    estimators=[\n",
    "        (\"lightgbm\", Light),\n",
    "        (\"xgboost\", XGBoost),\n",
    "        (\"randomforest\", RandomForest),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Submission1 = TrainML(voting_model, train_data, test_data)\n",
    "print(\"Submission:\", Submission1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.697855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2304\n",
      "[LightGBM] [Info] Number of data points in the train set: 1455563, number of used features: 500\n",
      "[LightGBM] [Info] Start training from score -3.597599\n",
      "[LightGBM] [Info] Start training from score -1.765956\n",
      "[LightGBM] [Info] Start training from score -2.347948\n",
      "[LightGBM] [Info] Start training from score -3.124281\n",
      "[LightGBM] [Info] Start training from score -1.037008\n",
      "[LightGBM] [Info] Start training from score -2.018196\n",
      "[LightGBM] [Info] Start training from score -3.396874\n",
      "[LightGBM] [Info] Start training from score -1.957809\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ÂÆö‰πâ LightGBM Ê®°Âûã\n",
    "model = LGBMClassifier(n_estimators=300, random_state=42)\n",
    "\n",
    "# ËÆ≠ÁªÉÊ®°Âûã\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ÊµãËØïÈõÜÈ¢ÑÊµã\n",
    "y_test_pred_lgbm = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# ÂÆö‰πâ CatBoostClassifier Ê®°Âûã\n",
    "model = CatBoostClassifier(iterations=300, random_state=42, verbose=0)\n",
    "\n",
    "# ËÆ≠ÁªÉÊ®°Âûã\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ÊµãËØïÈõÜÈ¢ÑÊµã\n",
    "\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ÊµãËØïÈõÜÈ¢ÑÊµã\n",
    "y_test_pred_rf = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰øùÂ≠òÈ¢ÑÊµãÁªìÊûú\n",
    "test_data[\"emotion\"] = y_test_pred_rf\n",
    "test_data[[\"tweet_id\", \"emotion\"]].to_csv(\"test_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
