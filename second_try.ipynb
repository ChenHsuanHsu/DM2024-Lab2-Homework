{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å°‡æ‰€æœ‰æ¬„ä½åˆ—å…¥åˆ†æåˆ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# å­˜å„²æå–æ•¸æ“šçš„åˆ—è¡¨\n",
    "tweets_data = []\n",
    "\n",
    "# è®€å– JSON æ–‡ä»¶\n",
    "with open('dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # è§£æ JSON æ¯è¡Œ\n",
    "            tweet = json.loads(line)\n",
    "            # æå–æ‰€éœ€çš„æ¬„ä½\n",
    "            tweet_id = tweet[\"_source\"][\"tweet\"].get(\"tweet_id\")\n",
    "            text = tweet[\"_source\"][\"tweet\"].get(\"text\")\n",
    "            hashtags = tweet[\"_source\"][\"tweet\"].get(\"hashtags\", [])\n",
    "            crawldate = tweet.get(\"_crawldate\")\n",
    "\n",
    "            # å°‡æå–çš„æ¬„ä½å­˜å…¥å­—å…¸ï¼Œä¸¦æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
    "            tweets_data.append({\n",
    "                \"tweet_id\": tweet_id,\n",
    "                \"text\": text,\n",
    "                \"hashtags\": hashtags,\n",
    "                \"crawldate\": crawldate\n",
    "            })\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"JSON decode error:\", e)\n",
    "\n",
    "# å°‡æå–çš„æ•¸æ“šè½‰æ›ç‚º DataFrame\n",
    "df = pd.DataFrame(tweets_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç°¡åŒ–first entryçš„è™•ç†æ–¹å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>2015-09-09 09:22:55</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>2015-10-10 14:33:26</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-10-23 08:49:50</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867525</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>\"For this is the message that ye heard from th...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-12-10 18:01:00</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>\"There is a lad here, which hath five barley l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-01-04 14:40:55</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "9        0x218443  When do you have enough ? When are you satisfi...   \n",
       "30       0x2939d5  God woke you up, now chase the day #GodsPlan #...   \n",
       "33       0x26289a  In these tough times, who do YOU turn to as yo...   \n",
       "...           ...                                                ...   \n",
       "1867525  0x2913b4  \"For this is the message that ye heard from th...   \n",
       "1867529  0x2a980e  \"There is a lad here, which hath five barley l...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "\n",
       "                                  hashtags            crawldate  \\\n",
       "2                             [bibleverse]  2017-12-25 04:39:20   \n",
       "4                                       []  2016-01-08 17:18:59   \n",
       "9        [materialism, money, possessions]  2015-09-09 09:22:55   \n",
       "30                    [GodsPlan, GodsWork]  2015-10-10 14:33:26   \n",
       "33                                      []  2016-10-23 08:49:50   \n",
       "...                                    ...                  ...   \n",
       "1867525                                 []  2016-12-10 18:01:00   \n",
       "1867529                                 []  2015-01-04 14:40:55   \n",
       "1867530    [mixedfeeling, butimTHATperson]  2015-05-12 12:51:52   \n",
       "1867531                                 []  2017-10-02 17:54:04   \n",
       "1867532                                 []  2016-10-10 11:04:32   \n",
       "\n",
       "        identification emotion  \n",
       "2                 test     NaN  \n",
       "4                 test     NaN  \n",
       "9                 test     NaN  \n",
       "30                test     NaN  \n",
       "33                test     NaN  \n",
       "...                ...     ...  \n",
       "1867525           test     NaN  \n",
       "1867529           test     NaN  \n",
       "1867530           test     NaN  \n",
       "1867531           test     NaN  \n",
       "1867532           test     NaN  \n",
       "\n",
       "[411972 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# å°‡æå–çš„ tweets_data è½‰æ›ç‚º DataFrame\n",
    "df = pd.DataFrame(tweets_data)\n",
    "\n",
    "# åŠ è¼‰å…¶ä»–æ•¸æ“šé›†\n",
    "classify = pd.read_csv('dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n",
    "emotion = pd.read_csv('dm-2024-isa-5810-lab-2-homework/emotion.csv')\n",
    "\n",
    "# åˆä½µ data_identification å’Œ emotion åˆ° df ä¸­\n",
    "data = df.merge(classify, on='tweet_id', how='outer').merge(emotion, on='tweet_id', how='outer')\n",
    "\n",
    "# æ ¹æ“š 'identification' æ¬„ä½åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦é›†\n",
    "train_data = data[data['identification'] == 'train']\n",
    "test_data = data[data['identification'] == 'test']\n",
    "\n",
    "# æå–æ‰€æœ‰å”¯ä¸€çš„æƒ…æ„Ÿæ¨™ç±¤\n",
    "emotions = train_data['emotion'].unique()\n",
    "\n",
    "# ä½¿ç”¨å­—å…¸å­˜å„²æŒ‰æƒ…æ„ŸåŠƒåˆ†çš„è¨“ç·´é›†å­ DataFrame\n",
    "train_data_by_emotion = {emotion: train_data[train_data['emotion'] == emotion] for emotion in emotions}\n",
    "\n",
    "# chceck the shape of testing data\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å°‡æœ‰hashtagsçš„æ¨æ–‡å¦å¤–è™•ç†ï¼Œå°‡æ‰€æœ‰hashtagsåˆä½µæˆå­—ä¸²åŠ å…¥æ¬„ä½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>2015-09-09 09:22:55</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>2015-10-10 14:33:26</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-10-23 08:49:50</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867525</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>\"For this is the message that ye heard from th...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-12-10 18:01:00</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>\"There is a lad here, which hath five barley l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-01-04 14:40:55</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "9        0x218443  When do you have enough ? When are you satisfi...   \n",
       "30       0x2939d5  God woke you up, now chase the day #GodsPlan #...   \n",
       "33       0x26289a  In these tough times, who do YOU turn to as yo...   \n",
       "...           ...                                                ...   \n",
       "1867525  0x2913b4  \"For this is the message that ye heard from th...   \n",
       "1867529  0x2a980e  \"There is a lad here, which hath five barley l...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "\n",
       "                                  hashtags            crawldate  \\\n",
       "2                             [bibleverse]  2017-12-25 04:39:20   \n",
       "4                                       []  2016-01-08 17:18:59   \n",
       "9        [materialism, money, possessions]  2015-09-09 09:22:55   \n",
       "30                    [GodsPlan, GodsWork]  2015-10-10 14:33:26   \n",
       "33                                      []  2016-10-23 08:49:50   \n",
       "...                                    ...                  ...   \n",
       "1867525                                 []  2016-12-10 18:01:00   \n",
       "1867529                                 []  2015-01-04 14:40:55   \n",
       "1867530    [mixedfeeling, butimTHATperson]  2015-05-12 12:51:52   \n",
       "1867531                                 []  2017-10-02 17:54:04   \n",
       "1867532                                 []  2016-10-10 11:04:32   \n",
       "\n",
       "        identification emotion  \n",
       "2                 test     NaN  \n",
       "4                 test     NaN  \n",
       "9                 test     NaN  \n",
       "30                test     NaN  \n",
       "33                test     NaN  \n",
       "...                ...     ...  \n",
       "1867525           test     NaN  \n",
       "1867529           test     NaN  \n",
       "1867530           test     NaN  \n",
       "1867531           test     NaN  \n",
       "1867532           test     NaN  \n",
       "\n",
       "[411972 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç¯©é¸åŒ…å« hashtags çš„æ¨æ–‡\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "# å°‡å«æœ‰ hashtags çš„è³‡æ–™ç¯©é¸å‡ºä¾†\n",
    "train_with_hashtags = train_data[train_data['hashtags'].notnull() & (train_data['hashtags'] != \"\")]\n",
    "train_without_hashtags = train_data[train_data['hashtags'].isnull() | (train_data['hashtags'] == \"\")]\n",
    "\n",
    "test_with_hashtags = test_data[test_data['hashtags'].notnull() & (test_data['hashtags'] != \"\")]\n",
    "test_without_hashtags = test_data[test_data['hashtags'].isnull() | (test_data['hashtags'] == \"\")]\n",
    "\n",
    "test_with_hashtags\n",
    "#train_with_hashtags\n",
    "\n",
    "# Here, the hashtags column is already tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.2 Save data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867526</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>2016-12-26 02:44:07</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867527</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-04-01 08:14:56</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867528</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>2016-11-17 23:46:22</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>   \n",
       "5        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1867526  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1867527  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1867528  0x2cbca6  there's currently two girls walking around the...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                              hashtags            crawldate identification  \\\n",
       "0                           [Snapchat]  2015-05-23 11:42:47          train   \n",
       "1        [freepress, TrumpLegacy, CNN]  2016-01-28 04:52:09          train   \n",
       "3                                   []  2016-01-24 23:53:05          train   \n",
       "5            [authentic, LaughOutLoud]  2015-06-11 04:44:05          train   \n",
       "6                                   []  2015-08-18 02:30:07          train   \n",
       "...                                ...                  ...            ...   \n",
       "1867526              [NoWonder, Happy]  2016-12-26 02:44:07          train   \n",
       "1867527                             []  2015-04-01 08:14:56          train   \n",
       "1867528                     [blessyou]  2016-11-17 23:46:22          train   \n",
       "1867533                             []  2016-09-02 14:25:06          train   \n",
       "1867534                  [Sundayvibes]  2016-11-16 01:40:07          train   \n",
       "\n",
       "              emotion  \n",
       "0        anticipation  \n",
       "1             sadness  \n",
       "3                fear  \n",
       "5                 joy  \n",
       "6        anticipation  \n",
       "...               ...  \n",
       "1867526           joy  \n",
       "1867527           joy  \n",
       "1867528           joy  \n",
       "1867533           joy  \n",
       "1867534           joy  \n",
       "\n",
       "[1455563 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# æŒ‡å®šè¼¸å‡ºè³‡æ–™å¤¾\n",
    "output_folder = 'output_2'\n",
    "\n",
    "# æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# æŒ‡å®šè¼¸å‡ºæª”æ¡ˆçš„å®Œæ•´è·¯å¾‘\n",
    "train_data_path = os.path.join(output_folder, \"train_data.pkl\")\n",
    "test_data_path = os.path.join(output_folder, \"test_data.pkl\")\n",
    "train_hashtags_path = os.path.join(output_folder, \"train_data_with_hashtags.pkl\")\n",
    "test_hashtags_path = os.path.join(output_folder, \"test_data_with_hashtags.pkl\")\n",
    "\n",
    "# å°‡ DataFrame å„²å­˜åˆ°æŒ‡å®šè·¯å¾‘çš„ pickle æ–‡ä»¶\n",
    "train_data.to_pickle(train_data_path)\n",
    "test_data.to_pickle(test_data_path)\n",
    "train_with_hashtags.to_pickle(train_hashtags_path)\n",
    "test_with_hashtags.to_pickle(test_hashtags_path)\n",
    "\n",
    "# è¼‰å…¥ä¿å­˜çš„ pickle æ–‡ä»¶é€²è¡Œé©—è­‰\n",
    "train_data = pd.read_pickle(train_data_path)\n",
    "test_data = pd.read_pickle(test_data_path)\n",
    "\n",
    "train_hashtags = pd.read_pickle(train_hashtags_path)\n",
    "test_hashtags = pd.read_pickle(test_hashtags_path)\n",
    "train_hashtags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.3 Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore data with hashtags\n",
    "æ¨æ¸¬hashtagæ¬„ä½æ‡‰è©²ä¹Ÿæœ‰å¾ˆå¤šç‰¹æ®Šç¬¦è™Ÿï¼ˆé¡ä¼¼emojiï¼‰ï¼Œå› æ­¤æ±ºå®šä½¿ç”¨éæ­·æ¯å€‹æ¨æ–‡ï¼Œå°‹æ‰¾æ˜¯å¦æœ‰å«è¡¨æƒ…ç¬¦è™Ÿä¹‹æ¨™è¨˜(hashtags)  \n",
    "**ç¶“éæŸ¥è­‰ï¼Œæ¨™è¨˜æ¬„ä½ä¸¦æœªåŒ…å«ä»»ä½•è¡¨æƒ…ç¬¦è™Ÿemoji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [tweet_id, text, hashtags, crawldate, identification, emotion, contains_emoji]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# è¡¨æƒ…ç¬¦è™Ÿçš„æ­£å‰‡è¡¨é”å¼æ¨¡å¼\n",
    "emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"  # è¡¨æƒ…ç¬¦è™Ÿ\n",
    "                           \"\\U0001F300-\\U0001F5FF\"  # ç¬¦è™Ÿå’Œåœ–å½¢\n",
    "                           \"\\U0001F680-\\U0001F6FF\"  # é‹è¼¸å’Œåœ°åœ–ç¬¦è™Ÿ\n",
    "                           \"\\U0001F1E0-\\U0001F1FF\"  # åœ‹æ——\n",
    "                           \"\\U00002700-\\U000027BF\"  # å…¶ä»–ç¬¦è™Ÿ\n",
    "                           \"\\U00002600-\\U000026FF\"  # å…¶ä»–åœ–å½¢ç¬¦è™Ÿ\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# è¼‰å…¥ train_hashtags.pkl æ–‡ä»¶\n",
    "train_with_hashtags = pd.read_pickle(\"output_2/train_data_with_hashtags.pkl\")\n",
    "\n",
    "# æª¢æŸ¥ hashtags æ¬„ä½æ˜¯å¦åŒ…å«è¡¨æƒ…ç¬¦è™Ÿï¼Œä¸¦ç¯©é¸å«æœ‰è¡¨æƒ…ç¬¦è™Ÿçš„è¡Œ\n",
    "train_with_hashtags['contains_emoji'] = train_with_hashtags['hashtags'].astype(str).apply(\n",
    "    lambda x: bool(emoji_pattern.search(x)) if pd.notnull(x) else False\n",
    ")\n",
    "\n",
    "# ç¯©é¸å«æœ‰è¡¨æƒ…ç¬¦è™Ÿçš„éƒ¨åˆ†\n",
    "emoji_rows = train_with_hashtags[train_with_hashtags['contains_emoji']]\n",
    "\n",
    "# æŸ¥çœ‹çµæœ\n",
    "print(emoji_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "    Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### é è™•ç†ï¼Œå› ç‚ºå–®ä¸€æ¨æ–‡å¯èƒ½æœ‰å¤šå€‹hashtags \n",
    "***list to string***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°‡æ¯å€‹ hashtags åˆ—è¡¨è½‰ç‚ºå–®ä¸€å­—ç¬¦ä¸²\n",
    "#train_hashtags['hashtags'] = train_hashtags['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "#train_hashtags['hashtags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for hashtags\n",
    "è©•ä¼°æ¨™è¨˜æ˜¯å¦æœ‰å°æ‡‰ç‰¹å®šæƒ…ç·’\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "## setting\n",
    "vector_dim = 1455563\n",
    "window_size = 5\n",
    "min_count = 1\n",
    "training_epochs = 20\n",
    "\n",
    "## model\n",
    "word2vec_model = Word2Vec(sentences=training_corpus, \n",
    "                          vector_size=vector_dim, window=window_size, \n",
    "                          min_count=min_count, epochs=training_epochs)\n",
    "\n",
    "# get the corresponding vector of a word\n",
    "word_vec = word2vec_model.wv['emotion']\n",
    "word_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœ X_train æ˜¯ä¸€ç¶­æ•¸çµ„ï¼Œè«‹é‡æ–°èª¿æ•´ç‚ºäºŒç¶­\n",
    "if X_train.ndim == 1:\n",
    "    X_train = X_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (100, 1)\n",
      "y_train shape: (1455563,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# å‡è¨­ word_vec æ˜¯ hashtags çš„å‘é‡è¡¨ç¤ºï¼Œtrain_hashtags['emotion'] æ˜¯æƒ…ç·’æ¨™ç±¤\n",
    "X_train = np.array(word_vec)  # å°‡ word_vec è½‰æ›ç‚ºæ•¸çµ„\n",
    "\n",
    "# å¦‚æœ X_train æ˜¯ä¸€ç¶­æ•¸çµ„ï¼Œè«‹é‡æ–°èª¿æ•´ç‚ºäºŒç¶­\n",
    "if X_train.ndim == 1:\n",
    "    X_train = X_train.reshape(-1, 1)\n",
    "\n",
    "y_train = np.array(train_hashtags['emotion'])  # å°‡æƒ…ç·’æ¨™ç±¤è½‰æ›ç‚ºæ•¸çµ„\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455563,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# å‡è¨­ train_hashtags['hashtags'] åŒ…å«åˆ—è¡¨ä¸­çš„åˆ—è¡¨\n",
    "# ç¢ºä¿ hashtags æ¬„ä½æ˜¯åˆ—è¡¨\n",
    "train_hashtags['token_hashtags'] = train_hashtags['hashtags'].apply(lambda x: nltk.word_tokenize(x))\n",
    "#train_hashtags[['tweet_id','hashtags','token_hashtags']]\n",
    "## create the training corpus\n",
    "training_corpus = train_hashtags['token_hashtags'].values\n",
    "#training_corpus.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–ä¸¦è¨“ç·´ Logistic Regression æ¨¡å‹\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# åˆå§‹åŒ–ä¸¦è¨“ç·´ Logistic Regression æ¨¡å‹\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ä½¿ç”¨æ¸¬è©¦é›†é€²è¡Œé æ¸¬\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "# è©•ä¼°æ¨¡å‹\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "report = classification_report(y_train, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>Snapchat</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>freepress TrumpLegacy CNN</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td></td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>authentic LaughOutLoud</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td></td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867526</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>NoWonder Happy</td>\n",
       "      <td>2016-12-26 02:44:07</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867527</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td></td>\n",
       "      <td>2015-04-01 08:14:56</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867528</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>blessyou</td>\n",
       "      <td>2016-11-17 23:46:22</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td></td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>Sundayvibes</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>   \n",
       "5        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1867526  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1867527  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1867528  0x2cbca6  there's currently two girls walking around the...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                          hashtags            crawldate identification  \\\n",
       "0                         Snapchat  2015-05-23 11:42:47          train   \n",
       "1        freepress TrumpLegacy CNN  2016-01-28 04:52:09          train   \n",
       "3                                   2016-01-24 23:53:05          train   \n",
       "5           authentic LaughOutLoud  2015-06-11 04:44:05          train   \n",
       "6                                   2015-08-18 02:30:07          train   \n",
       "...                            ...                  ...            ...   \n",
       "1867526             NoWonder Happy  2016-12-26 02:44:07          train   \n",
       "1867527                             2015-04-01 08:14:56          train   \n",
       "1867528                   blessyou  2016-11-17 23:46:22          train   \n",
       "1867533                             2016-09-02 14:25:06          train   \n",
       "1867534                Sundayvibes  2016-11-16 01:40:07          train   \n",
       "\n",
       "              emotion  \n",
       "0        anticipation  \n",
       "1             sadness  \n",
       "3                fear  \n",
       "5                 joy  \n",
       "6        anticipation  \n",
       "...               ...  \n",
       "1867526           joy  \n",
       "1867527           joy  \n",
       "1867528           joy  \n",
       "1867533           joy  \n",
       "1867534           joy  \n",
       "\n",
       "[1455563 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for smaller dataset\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: ç‰¹å¾µæå– (BOW)\n",
    "vectorizer = CountVectorizer(max_features=500)  # è¨­ç½®æœ€å¤§ç‰¹å¾µæ•¸ç‚º 500\n",
    "X_train = vectorizer.fit_transform(train_hashtags[\"hashtags\"])\n",
    "y_train = train_hashtags[\"emotion\"]\n",
    "\n",
    "train_hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´é›†æº–ç¢ºç‡: 0.41\n"
     ]
    }
   ],
   "source": [
    "# Step 2: å»ºç«‹å’Œè¨“ç·´ Naive Bayes æ¨¡å‹\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: æ¨¡å‹é æ¸¬\n",
    "y_train_pred = nb_model.predict(X_train)\n",
    "\n",
    "# Step 4: æ¨¡å‹æ€§èƒ½è©•ä¼°\n",
    "\n",
    "# æº–ç¢ºç‡\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"è¨“ç·´é›†æº–ç¢ºç‡: {acc_train:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.Evaluate and sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (with Hashtags) TF-IDF + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1455563x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 536183 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# å‡è¨­ train_hashtags['hashtags'] åŒ…å«åˆ—è¡¨ä¸­çš„åˆ—è¡¨\n",
    "train_hashtags['hashtags'] = train_hashtags['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "test_hashtags['hashtags'] = test_hashtags['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# åˆå§‹åŒ– TfidfVectorizerï¼Œå¹¶é™åˆ¶ç‰¹å¾æ•°ä¸º 1000\n",
    "tfidf_vect = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# ä½¿ç”¨ TF-IDF å‘é‡åŒ–å™¨è¿›è¡Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å‘é‡åŒ–\n",
    "train_tfidf = tfidf_vect.fit_transform(train_hashtags[\"hashtags\"])\n",
    "test_tfidf = tfidf_vect.fit_transform(test_hashtags[\"hashtags\"])\n",
    "\n",
    "# è·å–ç‰¹å¾åç§°åˆ—è¡¨\n",
    "feature_names_tf = tfidf_vect.get_feature_names_out()\n",
    "train_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy: 0.4243780585244335\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.65      0.05      0.10     39867\n",
      "anticipation       0.68      0.23      0.34    248935\n",
      "     disgust       0.57      0.12      0.20    139101\n",
      "        fear       0.65      0.08      0.13     63999\n",
      "         joy       0.39      0.95      0.55    516017\n",
      "     sadness       0.61      0.11      0.19    193437\n",
      "    surprise       0.83      0.08      0.15     48729\n",
      "       trust       0.68      0.11      0.19    205478\n",
      "\n",
      "    accuracy                           0.42   1455563\n",
      "   macro avg       0.63      0.22      0.23   1455563\n",
      "weighted avg       0.56      0.42      0.34   1455563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# å‡è¨­ X æ˜¯ hashtags çš„å‘é‡è¡¨ç¤ºï¼Œy æ˜¯æƒ…ç·’æ¨™ç±¤\n",
    "X_train = train_tfidf  # è«‹ç¢ºä¿ word_vec æ˜¯ä¸€å€‹ 2D æ•¸çµ„\n",
    "y_train = train_hashtags['emotion']  # å‡è¨­æƒ…ç·’æ¨™ç±¤åœ¨æ­¤æ¬„ä½\n",
    "\n",
    "# åˆå§‹åŒ–ä¸¦è¨“ç·´ Logistic Regression æ¨¡å‹\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ä½¿ç”¨è¨“ç·´é›†é€²è¡Œé æ¸¬\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "# è©•ä¼°æ¨¡å‹\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "report = classification_report(y_train, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of tf-idf & LogisticRegression\n",
    "y_pred_tfidfLR = model.predict(test_tfidf)\n",
    "y_pred_tfidfLR\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨å‰‡å‰µå»º\n",
    "output_folder = 'output_2'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_hashtags['tweet_id'],\n",
    "    'emotion': y_pred_tfidfLR\n",
    "})\n",
    "\n",
    "# å°‡ DataFrame ä¿å­˜ç‚º Parquet æ–‡ä»¶ï¼Œä¸åŒ…å«ç´¢å¼•åˆ—\n",
    "file_path = os.path.join(output_folder, \"prediction_hashtags.parquet\")\n",
    "result_df.to_parquet(\"prediction_hashtags.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (all data) Bag of Words + Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words (BOW)\n",
    "nltk_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1455563, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "train_data_BOW_features_500 = BOW_500.fit_transform(train_data['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1455563, 500)\n",
      "y_train.shape:  (1455563,)\n",
      "training accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = BOW_500.transform(train_data['text'])\n",
    "y_train = train_data['emotion']\n",
    "X_test = BOW_500.transform(test_data['text'])\n",
    "\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "\n",
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of BOW & DecisionTree\n",
    "y_pred_BOWDT = DT_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to parquet\n",
    "import pandas as pd\n",
    "import os\n",
    "# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨å‰‡å‰µå»º\n",
    "output_folder = 'output_2'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_hashtags['tweet_id'],\n",
    "    'emotion': y_pred_BOWDT\n",
    "})\n",
    "\n",
    "# å°‡ DataFrame ä¿å­˜ç‚º Parquet æ–‡ä»¶ï¼Œä¸åŒ…å«ç´¢å¼•åˆ—\n",
    "file_path = os.path.join(output_folder, \"prediction_all.parquet\")\n",
    "result_df.to_parquet(\"prediction_all.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5.combined & output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  emotion\n",
      "0  0x28b412  disgust\n",
      "1  0x2de201      joy\n",
      "2  0x218443      joy\n",
      "3  0x2939d5      joy\n",
      "4  0x26289a      joy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# åŒ¯å…¥å…©å€‹é æ¸¬çµæœ\n",
    "all_data_predictions = pd.read_parquet('prediction_all.parquet')  # åŒ…å«æ‰€æœ‰è³‡æ–™çš„é æ¸¬\n",
    "hashtag_data_predictions = pd.read_parquet('prediction_hashtags.parquet')  # åŒ…å«æœ‰ hashtags çš„é æ¸¬\n",
    "\n",
    "# åˆä½µå…©è€…ï¼Œå‡è¨­å…©è€…éƒ½åŸºæ–¼ç›¸åŒçš„ `id`\n",
    "combined_predictions = all_data_predictions.copy()\n",
    "\n",
    "\n",
    "# ä½¿ç”¨æœ‰ hashtags çš„æ•¸æ“šéƒ¨åˆ†è¦†è“‹æ‰€æœ‰æ•¸æ“šä¸­çš„å°æ‡‰éƒ¨åˆ†\n",
    "# å‡è¨­å…©å€‹ DataFrame ä¸­çš„ç´¢å¼•ä¸€è‡´\n",
    "combined_predictions.update(hashtag_data_predictions)\n",
    "\n",
    "print(combined_predictions.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# æŒ‡å®šè¼¸å‡ºè³‡æ–™å¤¾\n",
    "output_folder = 'output_2'\n",
    "# æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# æŒ‡å®šè¼¸å‡ºæª”æ¡ˆçš„å®Œæ•´è·¯å¾‘\n",
    "file_path = os.path.join(output_folder, \"final_predictions.parquet\")\n",
    "\n",
    "# å°‡ DataFrame å„²å­˜åˆ°æŒ‡å®šè·¯å¾‘çš„ pickle æ–‡ä»¶\n",
    "combined_predictions.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å‡è¨­æ‚¨æƒ³ç”¨ 70% ä¾†è‡ªæœ‰ hashtag çš„é æ¸¬ï¼Œ30% ä¾†è‡ªæ‰€æœ‰è³‡æ–™çš„é æ¸¬\n",
    "weight_hashtag = 0.3\n",
    "weight_all_data = 0.7\n",
    "all_data_predictions['emotion'] = pd.to_numeric(all_data_predictions['emotion'], errors='coerce')\n",
    "hashtag_data_predictions['emotion'] = pd.to_numeric(hashtag_data_predictions['emotion'], errors='coerce')\n",
    "\n",
    "# å°æ–¼æœ‰ hashtag çš„æ•¸æ“šï¼Œæ ¹æ“šæ¯”ä¾‹åˆä½µ\n",
    "for idx in hashtag_data_predictions.index:\n",
    "    combined_predictions.loc[idx, 'emotion'] = (\n",
    "        weight_hashtag * hashtag_data_predictions.loc[idx, 'emotion'] +\n",
    "        weight_all_data * all_data_predictions.loc[idx, 'emotion']\n",
    "    )\n",
    "output_folder = 'output_2'\n",
    "file_path = os.path.join(output_folder, \"final_predictions.parquet\")\n",
    "combined_predictions.to_parquet('final_predictions.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id emotion\n",
       "0       0x28b412    None\n",
       "1       0x2de201    None\n",
       "2       0x218443    None\n",
       "3       0x2939d5    None\n",
       "4       0x26289a    None\n",
       "...          ...     ...\n",
       "411967  0x2913b4    None\n",
       "411968  0x2a980e    None\n",
       "411969  0x316b80    None\n",
       "411970  0x29d0cb    None\n",
       "411971  0x2a6a4f    None\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd = pd.read_parquet('final_predictions.parquet')\n",
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡è¨­æƒ…ç·’æ¨™ç±¤å°æ‡‰çš„æ•¸å€¼\n",
    "emotion_map = {\n",
    "    'happy': 0,\n",
    "    'sad': 1,\n",
    "    'angry': 2,\n",
    "    'surprised': 3,\n",
    "    'neutral': 4\n",
    "}\n",
    "# åå‘æ˜ å°„ï¼Œå°‡æ•¸å€¼è½‰å›æƒ…ç·’æ¨™ç±¤\n",
    "reverse_emotion_map = {v: k for k, v in emotion_map.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
