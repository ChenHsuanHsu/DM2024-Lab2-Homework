{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將所有欄位列入分析列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 存儲提取數據的列表\n",
    "tweets_data = []\n",
    "\n",
    "# 讀取 JSON 文件\n",
    "with open('dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # 解析 JSON 每行\n",
    "            tweet = json.loads(line)\n",
    "            # 提取所需的欄位\n",
    "            tweet_id = tweet[\"_source\"][\"tweet\"].get(\"tweet_id\")\n",
    "            text = tweet[\"_source\"][\"tweet\"].get(\"text\")\n",
    "            hashtags = tweet[\"_source\"][\"tweet\"].get(\"hashtags\", [])\n",
    "            crawldate = tweet.get(\"_crawldate\")\n",
    "\n",
    "            # 將提取的欄位存入字典，並添加到列表中\n",
    "            tweets_data.append({\n",
    "                \"tweet_id\": tweet_id,\n",
    "                \"text\": text,\n",
    "                \"hashtags\": hashtags,\n",
    "                \"crawldate\": crawldate\n",
    "            })\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"JSON decode error:\", e)\n",
    "\n",
    "# 將提取的數據轉換為 DataFrame\n",
    "df = pd.DataFrame(tweets_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 簡化first entry的處理方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>2015-09-09 09:22:55</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>2015-10-10 14:33:26</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-10-23 08:49:50</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867525</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>\"For this is the message that ye heard from th...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-12-10 18:01:00</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>\"There is a lad here, which hath five barley l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-01-04 14:40:55</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "9        0x218443  When do you have enough ? When are you satisfi...   \n",
       "30       0x2939d5  God woke you up, now chase the day #GodsPlan #...   \n",
       "33       0x26289a  In these tough times, who do YOU turn to as yo...   \n",
       "...           ...                                                ...   \n",
       "1867525  0x2913b4  \"For this is the message that ye heard from th...   \n",
       "1867529  0x2a980e  \"There is a lad here, which hath five barley l...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "\n",
       "                                  hashtags            crawldate  \\\n",
       "2                             [bibleverse]  2017-12-25 04:39:20   \n",
       "4                                       []  2016-01-08 17:18:59   \n",
       "9        [materialism, money, possessions]  2015-09-09 09:22:55   \n",
       "30                    [GodsPlan, GodsWork]  2015-10-10 14:33:26   \n",
       "33                                      []  2016-10-23 08:49:50   \n",
       "...                                    ...                  ...   \n",
       "1867525                                 []  2016-12-10 18:01:00   \n",
       "1867529                                 []  2015-01-04 14:40:55   \n",
       "1867530    [mixedfeeling, butimTHATperson]  2015-05-12 12:51:52   \n",
       "1867531                                 []  2017-10-02 17:54:04   \n",
       "1867532                                 []  2016-10-10 11:04:32   \n",
       "\n",
       "        identification emotion  \n",
       "2                 test     NaN  \n",
       "4                 test     NaN  \n",
       "9                 test     NaN  \n",
       "30                test     NaN  \n",
       "33                test     NaN  \n",
       "...                ...     ...  \n",
       "1867525           test     NaN  \n",
       "1867529           test     NaN  \n",
       "1867530           test     NaN  \n",
       "1867531           test     NaN  \n",
       "1867532           test     NaN  \n",
       "\n",
       "[411972 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 將提取的 tweets_data 轉換為 DataFrame\n",
    "df = pd.DataFrame(tweets_data)\n",
    "\n",
    "# 加載其他數據集\n",
    "classify = pd.read_csv('dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n",
    "emotion = pd.read_csv('dm-2024-isa-5810-lab-2-homework/emotion.csv')\n",
    "\n",
    "# 合併 data_identification 和 emotion 到 df 中\n",
    "data = df.merge(classify, on='tweet_id', how='outer').merge(emotion, on='tweet_id', how='outer')\n",
    "\n",
    "# 根據 'identification' 欄位分割訓練和測試集\n",
    "train_data = data[data['identification'] == 'train']\n",
    "test_data = data[data['identification'] == 'test']\n",
    "\n",
    "# 提取所有唯一的情感標籤\n",
    "emotions = train_data['emotion'].unique()\n",
    "\n",
    "# 使用字典存儲按情感劃分的訓練集子 DataFrame\n",
    "train_data_by_emotion = {emotion: train_data[train_data['emotion'] == emotion] for emotion in emotions}\n",
    "\n",
    "# chceck the shape of testing data\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將有hashtags的推文另外處理，將所有hashtags合併成字串加入欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>2015-09-09 09:22:55</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>2015-10-10 14:33:26</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-10-23 08:49:50</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867525</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>\"For this is the message that ye heard from th...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-12-10 18:01:00</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>\"There is a lad here, which hath five barley l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-01-04 14:40:55</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "9        0x218443  When do you have enough ? When are you satisfi...   \n",
       "30       0x2939d5  God woke you up, now chase the day #GodsPlan #...   \n",
       "33       0x26289a  In these tough times, who do YOU turn to as yo...   \n",
       "...           ...                                                ...   \n",
       "1867525  0x2913b4  \"For this is the message that ye heard from th...   \n",
       "1867529  0x2a980e  \"There is a lad here, which hath five barley l...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "\n",
       "                                  hashtags            crawldate  \\\n",
       "2                             [bibleverse]  2017-12-25 04:39:20   \n",
       "4                                       []  2016-01-08 17:18:59   \n",
       "9        [materialism, money, possessions]  2015-09-09 09:22:55   \n",
       "30                    [GodsPlan, GodsWork]  2015-10-10 14:33:26   \n",
       "33                                      []  2016-10-23 08:49:50   \n",
       "...                                    ...                  ...   \n",
       "1867525                                 []  2016-12-10 18:01:00   \n",
       "1867529                                 []  2015-01-04 14:40:55   \n",
       "1867530    [mixedfeeling, butimTHATperson]  2015-05-12 12:51:52   \n",
       "1867531                                 []  2017-10-02 17:54:04   \n",
       "1867532                                 []  2016-10-10 11:04:32   \n",
       "\n",
       "        identification emotion  \n",
       "2                 test     NaN  \n",
       "4                 test     NaN  \n",
       "9                 test     NaN  \n",
       "30                test     NaN  \n",
       "33                test     NaN  \n",
       "...                ...     ...  \n",
       "1867525           test     NaN  \n",
       "1867529           test     NaN  \n",
       "1867530           test     NaN  \n",
       "1867531           test     NaN  \n",
       "1867532           test     NaN  \n",
       "\n",
       "[411972 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 篩選包含 hashtags 的推文\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "# 將含有 hashtags 的資料篩選出來\n",
    "train_with_hashtags = train_data[train_data['hashtags'].notnull() & (train_data['hashtags'] != \"\")]\n",
    "train_without_hashtags = train_data[train_data['hashtags'].isnull() | (train_data['hashtags'] == \"\")]\n",
    "\n",
    "test_with_hashtags = test_data[test_data['hashtags'].notnull() & (test_data['hashtags'] != \"\")]\n",
    "test_without_hashtags = test_data[test_data['hashtags'].isnull() | (test_data['hashtags'] == \"\")]\n",
    "\n",
    "test_with_hashtags\n",
    "#train_with_hashtags\n",
    "\n",
    "# Here, the hashtags column is already tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.2 Save data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867526</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>2016-12-26 02:44:07</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867527</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-04-01 08:14:56</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867528</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>2016-11-17 23:46:22</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>   \n",
       "5        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1867526  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1867527  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1867528  0x2cbca6  there's currently two girls walking around the...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                              hashtags            crawldate identification  \\\n",
       "0                           [Snapchat]  2015-05-23 11:42:47          train   \n",
       "1        [freepress, TrumpLegacy, CNN]  2016-01-28 04:52:09          train   \n",
       "3                                   []  2016-01-24 23:53:05          train   \n",
       "5            [authentic, LaughOutLoud]  2015-06-11 04:44:05          train   \n",
       "6                                   []  2015-08-18 02:30:07          train   \n",
       "...                                ...                  ...            ...   \n",
       "1867526              [NoWonder, Happy]  2016-12-26 02:44:07          train   \n",
       "1867527                             []  2015-04-01 08:14:56          train   \n",
       "1867528                     [blessyou]  2016-11-17 23:46:22          train   \n",
       "1867533                             []  2016-09-02 14:25:06          train   \n",
       "1867534                  [Sundayvibes]  2016-11-16 01:40:07          train   \n",
       "\n",
       "              emotion  \n",
       "0        anticipation  \n",
       "1             sadness  \n",
       "3                fear  \n",
       "5                 joy  \n",
       "6        anticipation  \n",
       "...               ...  \n",
       "1867526           joy  \n",
       "1867527           joy  \n",
       "1867528           joy  \n",
       "1867533           joy  \n",
       "1867534           joy  \n",
       "\n",
       "[1455563 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 指定輸出資料夾\n",
    "output_folder = 'output_2'\n",
    "\n",
    "# 檢查資料夾是否存在，如果不存在則創建\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 指定輸出檔案的完整路徑\n",
    "train_data_path = os.path.join(output_folder, \"train_data.pkl\")\n",
    "test_data_path = os.path.join(output_folder, \"test_data.pkl\")\n",
    "train_hashtags_path = os.path.join(output_folder, \"train_data_with_hashtags.pkl\")\n",
    "test_hashtags_path = os.path.join(output_folder, \"test_data_with_hashtags.pkl\")\n",
    "\n",
    "# 將 DataFrame 儲存到指定路徑的 pickle 文件\n",
    "train_data.to_pickle(train_data_path)\n",
    "test_data.to_pickle(test_data_path)\n",
    "train_with_hashtags.to_pickle(train_hashtags_path)\n",
    "test_with_hashtags.to_pickle(test_hashtags_path)\n",
    "\n",
    "# 載入保存的 pickle 文件進行驗證\n",
    "train_data = pd.read_pickle(train_data_path)\n",
    "test_data = pd.read_pickle(test_data_path)\n",
    "\n",
    "train_hashtags = pd.read_pickle(train_hashtags_path)\n",
    "test_hashtags = pd.read_pickle(test_hashtags_path)\n",
    "train_hashtags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.3 Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore data with hashtags\n",
    "推測hashtag欄位應該也有很多特殊符號（類似emoji），因此決定使用遍歷每個推文，尋找是否有含表情符號之標記(hashtags)  \n",
    "**經過查證，標記欄位並未包含任何表情符號emoji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [tweet_id, text, hashtags, crawldate, identification, emotion, contains_emoji]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 表情符號的正則表達式模式\n",
    "emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"  # 表情符號\n",
    "                           \"\\U0001F300-\\U0001F5FF\"  # 符號和圖形\n",
    "                           \"\\U0001F680-\\U0001F6FF\"  # 運輸和地圖符號\n",
    "                           \"\\U0001F1E0-\\U0001F1FF\"  # 國旗\n",
    "                           \"\\U00002700-\\U000027BF\"  # 其他符號\n",
    "                           \"\\U00002600-\\U000026FF\"  # 其他圖形符號\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# 載入 train_hashtags.pkl 文件\n",
    "train_with_hashtags = pd.read_pickle(\"output_2/train_data_with_hashtags.pkl\")\n",
    "\n",
    "# 檢查 hashtags 欄位是否包含表情符號，並篩選含有表情符號的行\n",
    "train_with_hashtags['contains_emoji'] = train_with_hashtags['hashtags'].astype(str).apply(\n",
    "    lambda x: bool(emoji_pattern.search(x)) if pd.notnull(x) else False\n",
    ")\n",
    "\n",
    "# 篩選含有表情符號的部分\n",
    "emoji_rows = train_with_hashtags[train_with_hashtags['contains_emoji']]\n",
    "\n",
    "# 查看結果\n",
    "print(emoji_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "    Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 預處理，因為單一推文可能有多個hashtags \n",
    "***list to string***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將每個 hashtags 列表轉為單一字符串\n",
    "#train_hashtags['hashtags'] = train_hashtags['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "#train_hashtags['hashtags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for hashtags\n",
    "評估標記是否有對應特定情緒\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "## setting\n",
    "vector_dim = 1455563\n",
    "window_size = 5\n",
    "min_count = 1\n",
    "training_epochs = 20\n",
    "\n",
    "## model\n",
    "word2vec_model = Word2Vec(sentences=training_corpus, \n",
    "                          vector_size=vector_dim, window=window_size, \n",
    "                          min_count=min_count, epochs=training_epochs)\n",
    "\n",
    "# get the corresponding vector of a word\n",
    "word_vec = word2vec_model.wv['emotion']\n",
    "word_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果 X_train 是一維數組，請重新調整為二維\n",
    "if X_train.ndim == 1:\n",
    "    X_train = X_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (100, 1)\n",
      "y_train shape: (1455563,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 假設 word_vec 是 hashtags 的向量表示，train_hashtags['emotion'] 是情緒標籤\n",
    "X_train = np.array(word_vec)  # 將 word_vec 轉換為數組\n",
    "\n",
    "# 如果 X_train 是一維數組，請重新調整為二維\n",
    "if X_train.ndim == 1:\n",
    "    X_train = X_train.reshape(-1, 1)\n",
    "\n",
    "y_train = np.array(train_hashtags['emotion'])  # 將情緒標籤轉換為數組\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455563,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 假設 train_hashtags['hashtags'] 包含列表中的列表\n",
    "# 確保 hashtags 欄位是列表\n",
    "train_hashtags['token_hashtags'] = train_hashtags['hashtags'].apply(lambda x: nltk.word_tokenize(x))\n",
    "#train_hashtags[['tweet_id','hashtags','token_hashtags']]\n",
    "## create the training corpus\n",
    "training_corpus = train_hashtags['token_hashtags'].values\n",
    "#training_corpus.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化並訓練 Logistic Regression 模型\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 初始化並訓練 Logistic Regression 模型\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 使用測試集進行預測\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "# 評估模型\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "report = classification_report(y_train, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>Snapchat</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>freepress TrumpLegacy CNN</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td></td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>authentic LaughOutLoud</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td></td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867526</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>NoWonder Happy</td>\n",
       "      <td>2016-12-26 02:44:07</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867527</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td></td>\n",
       "      <td>2015-04-01 08:14:56</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867528</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>blessyou</td>\n",
       "      <td>2016-11-17 23:46:22</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td></td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>Sundayvibes</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>   \n",
       "5        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1867526  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1867527  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1867528  0x2cbca6  there's currently two girls walking around the...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                          hashtags            crawldate identification  \\\n",
       "0                         Snapchat  2015-05-23 11:42:47          train   \n",
       "1        freepress TrumpLegacy CNN  2016-01-28 04:52:09          train   \n",
       "3                                   2016-01-24 23:53:05          train   \n",
       "5           authentic LaughOutLoud  2015-06-11 04:44:05          train   \n",
       "6                                   2015-08-18 02:30:07          train   \n",
       "...                            ...                  ...            ...   \n",
       "1867526             NoWonder Happy  2016-12-26 02:44:07          train   \n",
       "1867527                             2015-04-01 08:14:56          train   \n",
       "1867528                   blessyou  2016-11-17 23:46:22          train   \n",
       "1867533                             2016-09-02 14:25:06          train   \n",
       "1867534                Sundayvibes  2016-11-16 01:40:07          train   \n",
       "\n",
       "              emotion  \n",
       "0        anticipation  \n",
       "1             sadness  \n",
       "3                fear  \n",
       "5                 joy  \n",
       "6        anticipation  \n",
       "...               ...  \n",
       "1867526           joy  \n",
       "1867527           joy  \n",
       "1867528           joy  \n",
       "1867533           joy  \n",
       "1867534           joy  \n",
       "\n",
       "[1455563 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for smaller dataset\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: 特徵提取 (BOW)\n",
    "vectorizer = CountVectorizer(max_features=500)  # 設置最大特徵數為 500\n",
    "X_train = vectorizer.fit_transform(train_hashtags[\"hashtags\"])\n",
    "y_train = train_hashtags[\"emotion\"]\n",
    "\n",
    "train_hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集準確率: 0.41\n"
     ]
    }
   ],
   "source": [
    "# Step 2: 建立和訓練 Naive Bayes 模型\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: 模型預測\n",
    "y_train_pred = nb_model.predict(X_train)\n",
    "\n",
    "# Step 4: 模型性能評估\n",
    "\n",
    "# 準確率\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"訓練集準確率: {acc_train:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.Evaluate and sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (with Hashtags) TF-IDF + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1455563x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 536183 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 假設 train_hashtags['hashtags'] 包含列表中的列表\n",
    "train_hashtags['hashtags'] = train_hashtags['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "test_hashtags['hashtags'] = test_hashtags['hashtags'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# 初始化 TfidfVectorizer，并限制特征数为 1000\n",
    "tfidf_vect = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# 使用 TF-IDF 向量化器进行训练集和测试集的向量化\n",
    "train_tfidf = tfidf_vect.fit_transform(train_hashtags[\"hashtags\"])\n",
    "test_tfidf = tfidf_vect.fit_transform(test_hashtags[\"hashtags\"])\n",
    "\n",
    "# 获取特征名称列表\n",
    "feature_names_tf = tfidf_vect.get_feature_names_out()\n",
    "train_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Accuracy: 0.4243780585244335\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.65      0.05      0.10     39867\n",
      "anticipation       0.68      0.23      0.34    248935\n",
      "     disgust       0.57      0.12      0.20    139101\n",
      "        fear       0.65      0.08      0.13     63999\n",
      "         joy       0.39      0.95      0.55    516017\n",
      "     sadness       0.61      0.11      0.19    193437\n",
      "    surprise       0.83      0.08      0.15     48729\n",
      "       trust       0.68      0.11      0.19    205478\n",
      "\n",
      "    accuracy                           0.42   1455563\n",
      "   macro avg       0.63      0.22      0.23   1455563\n",
      "weighted avg       0.56      0.42      0.34   1455563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 假設 X 是 hashtags 的向量表示，y 是情緒標籤\n",
    "X_train = train_tfidf  # 請確保 word_vec 是一個 2D 數組\n",
    "y_train = train_hashtags['emotion']  # 假設情緒標籤在此欄位\n",
    "\n",
    "# 初始化並訓練 Logistic Regression 模型\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 使用訓練集進行預測\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "# 評估模型\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "report = classification_report(y_train, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of tf-idf & LogisticRegression\n",
    "y_pred_tfidfLR = model.predict(test_tfidf)\n",
    "y_pred_tfidfLR\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "# 確保資料夾存在，若不存在則創建\n",
    "output_folder = 'output_2'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_hashtags['tweet_id'],\n",
    "    'emotion': y_pred_tfidfLR\n",
    "})\n",
    "\n",
    "# 將 DataFrame 保存為 Parquet 文件，不包含索引列\n",
    "file_path = os.path.join(output_folder, \"prediction_hashtags.parquet\")\n",
    "result_df.to_parquet(\"prediction_hashtags.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (all data) Bag of Words + Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words (BOW)\n",
    "nltk_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1455563, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "train_data_BOW_features_500 = BOW_500.fit_transform(train_data['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1455563, 500)\n",
      "y_train.shape:  (1455563,)\n",
      "training accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = BOW_500.transform(train_data['text'])\n",
    "y_train = train_data['emotion']\n",
    "X_test = BOW_500.transform(test_data['text'])\n",
    "\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "\n",
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of BOW & DecisionTree\n",
    "y_pred_BOWDT = DT_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to parquet\n",
    "import pandas as pd\n",
    "import os\n",
    "# 確保資料夾存在，若不存在則創建\n",
    "output_folder = 'output_2'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_hashtags['tweet_id'],\n",
    "    'emotion': y_pred_BOWDT\n",
    "})\n",
    "\n",
    "# 將 DataFrame 保存為 Parquet 文件，不包含索引列\n",
    "file_path = os.path.join(output_folder, \"prediction_all.parquet\")\n",
    "result_df.to_parquet(\"prediction_all.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5.combined & output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  emotion\n",
      "0  0x28b412  disgust\n",
      "1  0x2de201      joy\n",
      "2  0x218443      joy\n",
      "3  0x2939d5      joy\n",
      "4  0x26289a      joy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 匯入兩個預測結果\n",
    "all_data_predictions = pd.read_parquet('prediction_all.parquet')  # 包含所有資料的預測\n",
    "hashtag_data_predictions = pd.read_parquet('prediction_hashtags.parquet')  # 包含有 hashtags 的預測\n",
    "\n",
    "# 合併兩者，假設兩者都基於相同的 `id`\n",
    "combined_predictions = all_data_predictions.copy()\n",
    "\n",
    "\n",
    "# 使用有 hashtags 的數據部分覆蓋所有數據中的對應部分\n",
    "# 假設兩個 DataFrame 中的索引一致\n",
    "combined_predictions.update(hashtag_data_predictions)\n",
    "\n",
    "print(combined_predictions.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 指定輸出資料夾\n",
    "output_folder = 'output_2'\n",
    "# 檢查資料夾是否存在，如果不存在則創建\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 指定輸出檔案的完整路徑\n",
    "file_path = os.path.join(output_folder, \"final_predictions.parquet\")\n",
    "\n",
    "# 將 DataFrame 儲存到指定路徑的 pickle 文件\n",
    "combined_predictions.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 假設您想用 70% 來自有 hashtag 的預測，30% 來自所有資料的預測\n",
    "weight_hashtag = 0.3\n",
    "weight_all_data = 0.7\n",
    "all_data_predictions['emotion'] = pd.to_numeric(all_data_predictions['emotion'], errors='coerce')\n",
    "hashtag_data_predictions['emotion'] = pd.to_numeric(hashtag_data_predictions['emotion'], errors='coerce')\n",
    "\n",
    "# 對於有 hashtag 的數據，根據比例合併\n",
    "for idx in hashtag_data_predictions.index:\n",
    "    combined_predictions.loc[idx, 'emotion'] = (\n",
    "        weight_hashtag * hashtag_data_predictions.loc[idx, 'emotion'] +\n",
    "        weight_all_data * all_data_predictions.loc[idx, 'emotion']\n",
    "    )\n",
    "output_folder = 'output_2'\n",
    "file_path = os.path.join(output_folder, \"final_predictions.parquet\")\n",
    "combined_predictions.to_parquet('final_predictions.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id emotion\n",
       "0       0x28b412    None\n",
       "1       0x2de201    None\n",
       "2       0x218443    None\n",
       "3       0x2939d5    None\n",
       "4       0x26289a    None\n",
       "...          ...     ...\n",
       "411967  0x2913b4    None\n",
       "411968  0x2a980e    None\n",
       "411969  0x316b80    None\n",
       "411970  0x29d0cb    None\n",
       "411971  0x2a6a4f    None\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd = pd.read_parquet('final_predictions.parquet')\n",
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假設情緒標籤對應的數值\n",
    "emotion_map = {\n",
    "    'happy': 0,\n",
    "    'sad': 1,\n",
    "    'angry': 2,\n",
    "    'surprised': 3,\n",
    "    'neutral': 4\n",
    "}\n",
    "# 反向映射，將數值轉回情緒標籤\n",
    "reverse_emotion_map = {v: k for k, v in emotion_map.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
